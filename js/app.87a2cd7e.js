(function(e){function t(t){for(var n,o,r=t[0],l=t[1],c=t[2],u=0,h=[];u<r.length;u++)o=r[u],s[o]&&h.push(s[o][0]),s[o]=0;for(n in l)Object.prototype.hasOwnProperty.call(l,n)&&(e[n]=l[n]);d&&d(t);while(h.length)h.shift()();return a.push.apply(a,c||[]),i()}function i(){for(var e,t=0;t<a.length;t++){for(var i=a[t],n=!0,r=1;r<i.length;r++){var l=i[r];0!==s[l]&&(n=!1)}n&&(a.splice(t--,1),e=o(o.s=i[0]))}return e}var n={},s={app:0},a=[];function o(t){if(n[t])return n[t].exports;var i=n[t]={i:t,l:!1,exports:{}};return e[t].call(i.exports,i,i.exports,o),i.l=!0,i.exports}o.m=e,o.c=n,o.d=function(e,t,i){o.o(e,t)||Object.defineProperty(e,t,{enumerable:!0,get:i})},o.r=function(e){"undefined"!==typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},o.t=function(e,t){if(1&t&&(e=o(e)),8&t)return e;if(4&t&&"object"===typeof e&&e&&e.__esModule)return e;var i=Object.create(null);if(o.r(i),Object.defineProperty(i,"default",{enumerable:!0,value:e}),2&t&&"string"!=typeof e)for(var n in e)o.d(i,n,function(t){return e[t]}.bind(null,n));return i},o.n=function(e){var t=e&&e.__esModule?function(){return e["default"]}:function(){return e};return o.d(t,"a",t),t},o.o=function(e,t){return Object.prototype.hasOwnProperty.call(e,t)},o.p="/algorithm-explorer/";var r=window["webpackJsonp"]=window["webpackJsonp"]||[],l=r.push.bind(r);r.push=t,r=r.slice();for(var c=0;c<r.length;c++)t(r[c]);var d=l;a.push([0,"chunk-vendors"]),i()})({0:function(e,t,i){e.exports=i("56d7")},"106f":function(e,t,i){},1640:function(e,t,i){},"2cba":function(e,t,i){"use strict";var n=i("ccd2"),s=i.n(n);s.a},"3eca":function(e,t,i){},"4ddf":function(e,t,i){"use strict";var n=i("d0d3"),s=i.n(n);s.a},5630:function(e,t,i){},"56d7":function(e,t,i){"use strict";i.r(t);i("cadf"),i("551c");var n=i("2b0e"),s=function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("div",{attrs:{id:"app"}},[i("x-particles",{staticClass:"particles",attrs:{config:e.particlesConfig}}),i("section",{ref:"scroller",staticClass:"scroller",on:{scroll:e.handleScroll}},[i("div",{staticClass:"intro",attrs:{tabindex:"-1"},on:{keyup:function(e){e.stopPropagation(),e.preventDefault()}}},[i("h1",{staticClass:"title is-spaced"},[e._v("Algorithm Explorer")]),i("h2",{staticClass:"subtitle"},[e._v("This framework explores machine learning techniques top down.")]),e._m(0),i("div",{staticClass:"start field is-grouped is-grouped-centered"},[i("div",{staticClass:"control"},[i("button",{staticClass:"button is-link",attrs:{tabindex:"0"},on:{click:e.start}},[e._v("\n            Start\n          ")])]),e._m(1)])]),i("div",{staticClass:"steps-container"},e._l(e.steps,function(t,n){return i("Step",{key:n,attrs:{step:n+1,"data-index":n,title:t.title}},[i("div",{attrs:{slot:"content"},slot:"content"},[t.description?i("Description",{attrs:{text:t.description}}):e._e(),t.cases?i("Cases",{attrs:{cases:t.cases}}):e._e(),t.optionsTitle?i("p",{staticClass:"title is-6"},[e._v(e._s(t.optionsTitle))]):e._e(),t.options?i("List",{attrs:{index:n,items:t.options,details:t.optionDetails,selected:e.treePath.length?e.treePath[n]:null,selectable:t.hasOwnProperty("children")},on:{onSelect:e.selectStep}}):e._e(),t.columns?i("Columns",{attrs:{columns:t.columns}}):e._e(),t.examples?i("Examples",{attrs:{examples:t.examples}}):e._e(),n===e.treeDepth?i("div",{staticClass:"again field is-grouped is-grouped-centered",style:{opacity:e.currentStep===e.treeDepth?"1":"0"}},[i("div",{staticClass:"control"},[i("button",{staticClass:"button is-text",attrs:{tabindex:"0"},on:{keyup:function(t){if(!("button"in t)&&e._k(t.keyCode,"enter",13,t.key,"Enter"))return null;t.stopPropagation(),t.preventDefault(),e.goToStep(e.currentStep-1)},click:function(t){e.goToStep(e.currentStep-1)}}},[e._v("\n                Go back\n              ")])]),i("p",[e._v("or")]),i("div",{staticClass:"control"},[i("button",{staticClass:"button is-text",attrs:{tabindex:"0"},on:{keyup:function(t){if(!("button"in t)&&e._k(t.keyCode,"enter",13,t.key,"Enter"))return null;e.goToStep(0,!0)},click:function(t){e.goToStep(0,!0)}}},[e._v("\n                explore more\n              ")])])]):e._e()],1)])})),i("Vocab",{attrs:{term:e.term,terms:e.terms,links:e.termLinks,position:e.vocabPos},on:{close:e.unsetTerm}})],1),i("Navigator",{class:{"slide-up":e.scrolled},attrs:{step:e.currentStep,total:e.treeDepth,"can-progress":e.canProgress},on:{back:function(t){e.goToStep(e.currentStep-1)},next:function(t){e.goToStep(e.currentStep+1)}}})],1)},a=[function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("div",{staticClass:"content"},[i("p",[e._v("\n          Rather than dive straight into algorithms, you begin with a question.\n          "),i("br"),i("strong",[e._v("What are you trying to solve?")])]),i("p",[e._v("It is designed to provide a clear overview on data science techniques and introduce the reader to a foundation of machine learning algorithms.")])])},function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("div",{staticClass:"control"},[i("div",{staticClass:"field-label is-small"},[i("label",{staticClass:"label"},[e._v("or press Enter")])])])}],o=(i("a481"),i("3b2b"),i("456d"),i("ac6a"),i("8afe")),r=(i("6762"),i("2fdb"),i("f13c")),l=i.n(r),c=i("0f32"),d=i.n(c),u=i("7528"),h=i.n(u),p=i("71d6"),m=[{title:"Linear Regression",description:"Linear regression attempts to fit a straight hyperplane to your dataset that is closest to all data points. It is most suitable when there are linear relationships between the variables in the dataset.",examples:[{text:"Python",href:"http://nbviewer.jupyter.org/github/srnghn/ml_example_notebooks/blob/master/Predicting%20Yacht%20Resistance%20with%20Linear%20Regression.ipynb"}],columns:[{title:"Pros",options:["Quick to compute and can be updated easily with new data","Relatively easy to understand and explain","Regularization techniques can be used to prevent overfitting"]},{title:"Cons",options:["Unable to learn complex relationships","Difficult to capture non-linear relationships (without first transforming data which can be complicated)"]}]},{title:"Decision Trees",description:"Decision trees learn how to best split the dataset into separate branches, allowing it to learn non-linear relationships. <br><br>Random forests (RF) and Gradient Boosted Trees (GBT) are two algorithms that build many individual decision trees, pooling their predictions. As they use a collection of results to make a final decision, they are referred to as 'Ensemble techniques'.",examples:[{text:"Python",href:"http://nbviewer.jupyter.org/github/srnghn/ml_example_notebooks/blob/master/Predicting%20Yacht%20Resistance%20with%20Decision%20Trees%20%26%20Random%20Forests.ipynb"}],columns:[{title:"Pros",options:["A single decision tree is fast to train","Robust to noise/outliers and missing values",'RF perform very well “out-of-the-box"']},{title:"Cons",options:["Single decision trees are prone to overfitting (which is where ensembles come in!)","Complex trees are hard to interpret"]}]},{title:"Neural Networks",description:"Neural networks can learn complex patterns using layers of neurons which mathematically transform the data. The layers between the input and output are referred to as “hidden layers”. A neural network can learn relationships between the features that other algorithms cannot easily discover.",examples:[{text:"Python",href:"http://nbviewer.jupyter.org/github/srnghn/ml_example_notebooks/blob/master/Predicting%20Yacht%20Resistance%20with%20Neural%20Networks.ipynb"}],columns:[{title:"Pros",options:["Extremely powerful / state of the art for many domains (e.g. computer vision, speech recognition)","Can learn even very complex relationships","Hidden layers reduce need for feature engineering (no need to understand underlying data)"]},{title:"Cons",options:["Require a very large amount of data!","Prone to overfitting","Long training time","Requires significant computing power for large datasets (computationally expensive)",'Model is a "black box", unexplainable']}]},{title:"K-Nearest Neighbor",description:"K-Nearest Neighbor (KNN) makes a prediction for a new observation by searching for the most similar training observations and pooling their values.",examples:[{text:"Python",href:"http://nbviewer.jupyter.org/github/srnghn/ml_example_notebooks/blob/master/Predicting%20Yacht%20Resistance%20with%20K%20Nearest%20Neighbors.ipynb"}],columns:[{title:"Pros",options:["Simple","Powerful","No training involved"]},{title:"Cons",options:["Expensive & slow to predict new instances","Performs poorly on high dimensional datasets"]}]}],f=[{title:"K-means",description:"K-Means Clustering aims to partition the observations into k clusters. The algorithm will determine which observation is in which cluster and also the center of each cluster. A new observation is assiged the cluster whose center it is nearest to.",columns:[{title:"Pros",options:["Simple and easy to implement","Easy to interpret results","Fast"]},{title:"Cons",options:["Sensitive to outliers","You must define the number of clusters","Assumes the clusters are spherical","The clusters are found using a random starting point so may not be repeatable and can require multiple runs to find an optimal solutions"]}]},{title:"Gaussian Mixture Model",description:"With a Gaussian Mixture Model (GMM), we are assuming that the k clusters are of normal distribution. It's algorithm tries to find the mean and standard deviation of each of these clusters. For a new observation, the probability it belongs to each cluster is calculated, resulting in a soft assignment.",columns:[{title:"Pros",options:["Does not enforce the clusters to be circular","Points can be assigned to multiple clusters"]},{title:"Cons",options:["You must define the number of clusters","Difficult to interpret"]}]},{title:"DBSCAN",description:"Density-Based Spatial Clustering of Applications with Noise (DBSCAN) attempts to find dense areas of data points and identifies these as a cluster. If data points are close enough to each other, and there are a sufficient number of them, they form a cluster. If not, they are labeled as noise and ignored.",columns:[{title:"Pros",options:["Can find arbitrarily shaped clusters","Does not require defining the number of clusters","Robust to outliers"]},{title:"Cons",options:["Cannot cluster datasets with large differences in densities","Can perform poorly on high dimensional data","Choosing the right parameters for density can be difficult"]}]},{title:"Agglomerative Hierarchical Clustering",description:"Agglomerative hierarchical clustering is an algorithm that builds a hierarchy of clusters. Initially all points start as their own cluster, then the two nearest clusters merge as the same cluster. This process continues, clusters merging until only one cluster containing all the data points remains. To identify the significant clusters a threshold would be chosen.",columns:[{title:"Pros",options:["Easy probabilistic model with interpretable topics","Does not require defining the number of clusters","Clusters can be arbitrarily shaped"]},{title:"Cons",options:["Slow and therefore not suitable for big data","Can be hard to identify the correct number of clusters","Interpretation can be confusing"]}]}],g=[{title:"Support Vector Machines",description:"If you plot your data in an n-dimensional space (where n is the number of features), Support Vector Machines (SVM) attempt to fit a hyperplane that best separates the categories. When you have a new data point, its position in relation to the hyperplane will predict which category the point belongs to.",examples:[{text:"Python",href:"http://nbviewer.jupyter.org/github/srnghn/ml_example_notebooks/blob/master/Predicting%20Wine%20Types%20with%20SVM.ipynb"}],columns:[{title:"Pros",options:["High accuracy","Ability to find solutions even if non-linearly separable","Good for high-dimensional space (lots of features)"]},{title:"Cons",options:["Hard to interpret","Can be slow to train large data sets","Memory-intensive"]}]},{title:"Naïve Bayes",description:'Naïve Bayes assumes that all features are independent, that they independently contribute to the probability of the target variable\'s class; this does not always hold true which is why it is referred to as "Naive". Various probabilities and likelihood values are calculated based upon the frequency they appear in the data and the final probabilities calculated using a formula called Bayes Theorem.',examples:[{text:"Python",href:"http://nbviewer.jupyter.org/github/srnghn/ml_example_notebooks/blob/master/Predicting%20Wine%20Types%20with%20Naive%20Bayes.ipynb"}],columns:[{title:"Pros",options:["Simple and easy to interpret","Computationally fast","Good for high-dimensional space (lots of features)"]},{title:"Cons",options:["Performance will be inhibited if significant dependence between variables","If a class that appears in the test data did not appear in the training data, it will be given a probability of zero"]}]},{title:"Logistic Regression",description:"Logistic regression predicts the probability of a binary outcome. A new observation is predicted to be within the class if its probability is above a set threshold. There are methods to use Logistic Regression for scenarios where there are multiple classes.",examples:[{text:"Python",href:"http://nbviewer.jupyter.org/github/srnghn/ml_example_notebooks/blob/master/Predicting%20Wine%20Types%20with%20Logistic%20Regression.ipynb"}],columns:[{title:"Pros",options:["Quick to compute and can be updated easily with new data","Output can be interpreted as probability; this can be used for ranking","Regularization techniques can be used to prevent overfitting"]},{title:"Cons",options:["Unable to learn complex relationships","Difficult to capture non-linear relationships (without first transforming data which can be complicated)"]}]},{title:"Decision Trees (inc. Random Forest & GBT)",description:"Decision trees learn how to best split the dataset into separate branches, allowing it to learn non-linear relationships. <br><br>Random forests (RF) and Gradient Boosted Trees (GBT) combine predictions from many individual decision trees. As they use a collection of results to make a final decision, they are referred to as 'Ensemble techniques'.",examples:[{text:"Python",href:"http://nbviewer.jupyter.org/github/srnghn/ml_example_notebooks/blob/master/Predicting%20Wine%20Types%20with%20Decision%20Trees%20%26%20Random%20Forest.ipynb"}],columns:[{title:"Pros",options:["A single decision tree is fast to train ","Robust to noise/outliers and missing values ","Random Forests perform very well “out-of-the-box”"]},{title:"Cons",options:["Single decision trees are prone to overfitting (which is where ensembles come in!)","Complex trees are hard to interpret","Gradient Boosted Trees have a lot of hyper-parameters to tune and are also prone to overfitting."]}]},{title:"Neural Networks",description:"Neural networks can learn complex patterns using layers of neurons which mathematically transform the data. The layers between the input and output are referred to as “hidden layers”. A neural network can learn relationships between the features that other algorithms cannot easily discover.",examples:[{text:"Python",href:"http://nbviewer.jupyter.org/github/srnghn/ml_example_notebooks/blob/master/Predicting%20Wine%20Types%20with%20Neural%20Networks.ipynb"}],columns:[{title:"Pros",options:["Extremely powerful / state of the art for many domains (e.g. computer vision, speech recognition)","Can learn even very complex relationships","Hidden layers reduce need for feature engineering (no need to understand underlying data)"]},{title:"Cons",options:["Require a very large amount of data!","Prone to overfitting","Long training time","Requires significant computing power for large datasets (computationally expensive)",'Model is a "black box", unexplainable']}]}],b=[{title:"K-Means",description:'Clustering techniques are a common approach for anomaly detection. Clusters of "normal" characteristics are identified and if the distance between a new point and all other clusters is too great, it is identified as an anomaly. <br><br>K-Means Clustering aims to partition n observations (data points) into k clusters in which each observation belongs to the cluster with the nearest center. <br><br>For more examples of Clustering, please chose to explore more techniques below and select "Discover structure in your data"',columns:[{title:"Pros",options:["Simple and easy to implement","Easy to interpret results","Fast"]},{title:"Cons",options:["Sensitive to outliers","You must define the number of clusters","Assumes the clusters are spherical","The clusters are found using a random starting point so may not be repeatable and can require multiple runs to find an optimal solutions"]}]},{title:"One-Class Support Vector Machines",description:'If you were to plot your data in an n-dimensional space (where n is the number of features), One-Class Support Vector Machines (SVM) attempt to identify the region where most cases lie, these are considered "normal". It will then fit a hyperplane that best separates these "normal" examples from the rest. When you have a new data point, it is labeled as "normal" or an "anomaly" depending how close it is to the "normal" boundary',columns:[{title:"Pros",options:["No assumptions on the distribution of the data","Ability to find normal boundary that is non-linear","Can be used in high-dimensional space"]},{title:"Cons",options:["Choosing the right hyper-parameters to find the appropriate non-linear shape of the boundary can be difficult","Can be slow to train large datasets","Memory intensive"]}]},{title:"Autoencoder",description:'An Autoencoder is a technique for dimensionality reduction. It is a type of neural network where the first part of the network, called the encoder, reduces the input to a lower dimension. The second part of the network, called the decoder, aims to reconstruct the original input. The goal is to create a model where the input and the output are the same. A new data point can be passed through the model and if the error between the input data and the computed output is too great, it can be flagged as an "anomaly".',examples:[{text:"Python"},{text:"Keras"}],columns:[{title:"Pros",options:["Can capture non-linearities and subtle connections between the features","Variations result in state-of-the-art results","If the data is temporal, LSTM (long-short-term-memory) autoencoders can be used"]},{title:"Cons",options:["Requires a very large amount of data","Many hyperparameters to tune","Long training time","Requires significant computing power for large datasets "]}]}],y=[{title:"Content-based Recommenders",description:"Content-based recommenders suggest similar items to those already liked by the customer, whether explicitly (e.g. by rating) or implicitly (by purchasing). This type of system uses metadata describing the item.<br><br> Each item is represented as a vector and a distance metric compares the items' vectors to find the most similar.",columns:[{title:"Pros",options:["Quick to implement","No popularity bias - can recommend new items","Results are interpretable"]},{title:"Cons",options:["Important to have meaningful metadata, tagging can be tiresome",'"Cold start problem" for new users without history of liking items',"Limits novelty as won't suggest items too disimilar to previously liked"]}]},{title:"Memory-based Collaborative Filtering",description:"Collaborative filtering is a method of predicting a user's interest by analysing preferences by other users. There are two types, user-based filtering and item-based filtering. <br><br>Memory-based filtering computes similarity between users, or items, and uses other users' ratings to make a prediction; a typical approach is a neighborhood-based algorithm. The predicted rating for the user and each item is calcuated using other users' ratings and the similarity distance",columns:[{title:"Pros",options:["Quick to implement","Results are interpretable","User based suggestions can result in a diverse set of suggestions across domains"]},{title:"Cons",options:["Data sparsity can result in performance issues","Slow & computationally expensive - requires the whole dataset to make a prediction",'"Cold start problem" - new items struggle to be recommended (popularity bias) and for new users with little history it\'s hard to make recommendations']}]},{title:"Model-based Collaborative Filtering",description:"Collaborative filtering is a method of predicting a user's interest by analysing preferences by other users. There are two types, user-based filtering and item-based filtering.  <br><br>Model-based recommenders use training data to build a model, a mathematical formula that takes the features of an observation and calculates a prediction. There are many algorithms to use, including neural networks, Bayesian networks and matrix factorization.",examples:[{text:"Python"},{text:"SparkML"},{text:"Keras"}],columns:[{title:"Pros",options:["Fast & scalable; doesn't require the full dataset each time","User-based suggestions can result in a diverse set of suggestions across domains","User-based suggestions do not require metadata"]},{title:"Cons",options:["Data sparsity can result in performance issues","Models can be complex and slow to train",'"Cold start problem" - new items struggle to be recommended (popularity bias) and for new users with on history its hard to make good recommendations']}]}],v="Most common algorithms",w=[{title:"It looks like you should use <strong>Regression Techniques</strong>",description:"Regression algorithms are machine learning techniques for predicting continuous numerical values. They are supervised learning tasks which means they require labelled training examples.",cases:["Predicting the appropriate price for a product based upon size, brand, and location.","Predicting the number of sales each day based upon store location, public holidays, day of the week and the closest competitor store."],optionsTitle:v,options:["Linear Regression","Decision Trees","Neural Networks","K-Nearest Neighbors"],children:m},{title:"It looks like you should use <strong>Classification Techniques</strong>",description:"Classification algorithms are machine learning techniques for predicting which category the input data belongs to. They are supervised learning tasks which means they require labelled training examples.",cases:["Predicting a clinical diagnosis based upon symptoms, laboratory results and historical diagnosis.","Predicting whether a healthcare claim is fraudulent using data such as claim amount, drug predisposition, disease and provider"],optionsTitle:v,options:["Support Vector Machines","Naïve Bayes","Logistic Regression","Decision Trees","Neural Networks"],children:g},{title:"It looks like you should use <strong>Clustering Techniques</strong>",description:"Clustering algorithms are machine learning techniques to divide data into a number of groups where points in groups have similar traits. They are unsupervised learning tasks and therefore do not require labelled training examples.",cases:["Segmenting your market based upon similar collections of customers using their location, spending habits and demographics.","Understanding topics in your documents, whether they are emails, reports, or customer call transactions by exploring the common words"],optionsTitle:v,options:["K-Means","Gaussian Mixture Model","DBSCAN","Agglomerative Hierarchical Clustering"],children:f},{title:"It looks like you should use <strong>Recommendation Engine Techniques</strong>",description:"Recommendation Engines are created in order to predict a preference or rating that indicates a users interest in an item/product. The algorithms used to create this system find similarities between either the users, the items or both.",cases:["Recommend clothing to a customer based on brands, colors, and price of previously purchased clothing","Recommend a medical treatment for a patient based upon successful treatments given to similar patients using their condition, diagnosis and previous treatment information"],optionsTitle:v,options:["Content-based Recommenders","Memory-based Collaborative Filtering","Model-based Collaborative Filtering"],children:y},{title:"It looks like you should use <strong>Anomaly Detection Techniques</strong>",description:"Anomaly Detection is a technique used to identify unusual events or patterns that do not conform to expected behavior. Those identified are often referred to as anomalies or outliers.",cases:["Detect abnormal behavior of equipment in a manufacturing plant using sensor data such as temperature, pressure and humidity","Detect and prevent fraudulent spending by understanding normal customer spending amounts, locations and time between taransactions"],optionsTitle:v,options:["K-Means","One-class Support Vector Machines","Autoencoders"],children:b}],x={title:"Are you trying to...",options:["Predict a numerical outcome?","Predict a categorical outcome?","Discover structure in your data?","Make recommendations?","Detect outliers or unusual behavior?"],optionDetails:[["Predicting the appropriate price for a product","Predicting the number of sales each day"],["Predicting a clinical diagnosis","Predicting whether a healthcare claim is fraudulent or not"],["Segment your market based upon similar collections of customers","Understanding topics (e.g. payments, delivery, returns) in your customer emails"],["Recommend clothing based upon previous purchases","Recommend a treatment based upon success for similar patients"],["Detect abnormal behaviour of equipment in a manufacturing plant","Detect and prevent fraudulent spending"]],children:w},k=i("d9a3"),C={model:"https://en.wikipedia.org/wiki/Statistical_model",pooling:"https://en.wikipedia.org/wiki/Pooled_variance",vector:"https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)","non-linear relationships":"https://en.wikipedia.org/wiki/Nonlinear_system",independent:"https://en.wikipedia.org/wiki/Independence_(probability_theory)",cluster:"https://en.wikipedia.org/wiki/Cluster_analysis#cluster"},_=function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("div",{staticClass:"cases"},[i("p",{staticClass:"title is-6"},[e._v("Cases")]),i("ul",e._l(e.cases,function(t,n){return i("li",{key:n,domProps:{innerHTML:e._s(t)}})}))])},T=[],P={name:"Cases",props:{cases:{type:Array,default:function(){return[]}}}},S=P,A=(i("2cba"),i("2877")),N=Object(A["a"])(S,_,T,!1,null,"7a4945ce",null),R=N.exports,M=R,D=function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("div",{staticClass:"columns is-variable is-5"},e._l(e.columns,function(t,n){return i("div",{key:n,staticClass:"column",class:n%2?"cons":"pros"},[i("p",{staticClass:"title is-6"},[e._v(e._s(t.title))]),i("ul",e._l(t.options,function(t,n){return i("li",{key:n,domProps:{innerHTML:e._s(t)}})}))])}))},q=[],E={name:"Columns",props:{columns:{type:Array,default:function(){return[]}}}},L=E,j=(i("8b4e"),Object(A["a"])(L,D,q,!1,null,"028ad5d0",null)),I=j.exports,O=I,z=function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("div",{staticClass:"examples"},[i("p",{staticClass:"title is-6"},[e._v("Examples")]),i("ul",{staticClass:"examples-list"},e._l(e.examples,function(t,n){var s=t.href,a=t.text;return i("li",{key:n,staticClass:"example"},[i("a",{attrs:{href:s,target:"_blank"}},[e._v("\n        "+e._s(a)+"\n        "),i("font-awesome-icon",{staticClass:"example-icon",attrs:{icon:"link",size:"xs"}})],1)])}))])},B=[],H={name:"Examples",props:{examples:{type:Array,default:function(){return[]}}}},F=H,$=(i("6a03"),Object(A["a"])(F,z,B,!1,null,"1c24b784",null)),K=$.exports,V=K,W=function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("div",[i("p",{staticClass:"description",domProps:{innerHTML:e._s(e.text)}})])},G=[],U={name:"Description",props:{text:{type:String,default:""}}},Y=U,Q=(i("4ddf"),Object(A["a"])(Y,W,G,!1,null,null,null)),J=Q.exports,X=J,Z=function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("ul",{class:{list:!0,selectable:e.selectable}},e._l(e.items,function(t,n){return i("li",{key:n,class:["list-item",e.selected===n&&"selected"],attrs:{tabindex:"0"},on:{focus:function(t){e.activeInfo=null},keyup:function(t){if(!("button"in t)&&e._k(t.keyCode,"enter",13,t.key,"Enter"))return null;e.select(n)},click:function(t){e.select(n)}}},[e.selectable?i("div",{staticClass:"list-letter is-uppercase"},[e._v("\n      "+e._s(String.fromCharCode(65+n))+"\n    ")]):e._e(),i("div",{staticClass:"item-content"},[i("span",{domProps:{innerHTML:e._s(e.isObject?t[e.textKey]:t)}})]),e.details&&e.details[n]?i("div",{class:["dropdown is-right",n===e.activeInfo&&"is-active"]},[i("div",{staticClass:"dropdown-trigger",attrs:{tabindex:"-1"},on:{click:function(t){t.stopPropagation(),t.preventDefault(),e.activeInfo=null===e.activeInfo?n:null}}},[i("font-awesome-icon",{staticClass:"more-info",attrs:{icon:"info-circle","aria-haspopup":"true","aria-controls":"dropdown-menu"}})],1),i("div",{staticClass:"dropdown-menu",attrs:{id:"dropdown-menu",role:"menu"}},[i("div",{staticClass:"dropdown-content"},[i("div",{staticClass:"dropdown-item"},[i("ul",{staticClass:"item-details"},[i("li",[e._v("For example")]),e._l(e.details[n],function(t,n){return i("li",{key:n,staticClass:"item-detail",domProps:{innerHTML:e._s(t)}})})],2)])])])]):e._e()])}))},ee=[],te=i("6bde"),ie=(i("c5f6"),{name:"List",props:{items:{type:Array,default:function(){return[]}},details:{type:Array,default:function(){return[]}},textKey:{type:String,default:"text"},index:{type:Number,default:0},selected:{type:Number||null,default:null},selectable:{type:Boolean,default:!1}},data:function(){return{activeInfo:null}},computed:{isObject:function(){return this.items.length&&"object"===Object(te["a"])(this.items[0])}},mounted:function(){window.addEventListener("click",this.handleClick)},destroyed:function(){window.removeEventListener("click",this.handleClick)},methods:{select:function(e){this.selectable&&this.$emit("onSelect",{parentIdx:this.index,selectedIdx:e})},handleClick:function(e){var t=["svg","path"];t.includes(e.target.tagName)||(this.activeInfo=null)}}}),ne=ie,se=(i("f9b4"),Object(A["a"])(ne,Z,ee,!1,null,"2f88fb60",null)),ae=se.exports,oe=ae,re=function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("div",{staticClass:"navigator has-text-white"},[i("div",{staticClass:"navigator-container"},[i("div",{staticClass:"progress-percent"},[e._v(e._s(e.currentProgress)+" complete")]),i("div",{staticClass:"progress"},[i("div",{staticClass:"current-progress",style:{width:e.currentProgress}})])]),i("div",{staticClass:"controls"},[i("font-awesome-icon",{staticClass:"control",class:!e.hasBack&&"disabled",attrs:{icon:"chevron-up"},on:{click:e.back}}),i("font-awesome-icon",{staticClass:"control",class:!e.hasNext&&"disabled",attrs:{icon:"chevron-down"},on:{click:e.next}})],1)])},le=[],ce={name:"Navigator",props:{step:{type:Number,default:0},total:{type:Number,default:1},canProgress:{type:Boolean,default:!1}},computed:{currentProgress:function(){return"".concat(Math.round(this.step/this.total*100),"%")},hasBack:function(){return 0!==this.step},hasNext:function(){return this.canProgress&&this.step!==this.total}},methods:{back:function(){this.hasBack&&this.$emit("back")},next:function(){this.hasNext&&this.$emit("next")}}},de=ce,ue=(i("ba17"),Object(A["a"])(de,re,le,!1,null,"0aca85cc",null)),he=ue.exports,pe=he,me=function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("div",{staticClass:"step",attrs:{tabindex:"-1"},on:{keyup:function(e){e.stopPropagation(),e.preventDefault()}}},[e.title?i("header",[i("span",{staticClass:"step-number"},[e._v(e._s(e.step)+". ")]),i("span",{staticClass:"step-title",domProps:{innerHTML:e._s(e.title)}})]):e._e(),i("div",{staticClass:"step-content"},[e._t("content")],2),e.$slots.footer?i("footer",{staticClass:"step-footer"},[e._t("footer")],2):e._e()])},fe=[],ge={name:"Step",props:{title:{type:String,default:""},step:{type:Number,default:1}}},be=ge,ye=(i("c19f"),Object(A["a"])(be,me,fe,!1,null,null,null)),ve=ye.exports,we=ve,xe=function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("transition",{attrs:{name:"fade"}},[e.term?i("div",[i("div",{staticClass:"vocab__box",style:{top:e.boxTop,left:e.boxLeft}},[e._v("\n      "+e._s(e.description)+"\n      "),i("div",{staticClass:"vocab__link"},[i("a",{attrs:{target:"_blank",href:e.link}},[e._v("\n          Learn more\n        ")])])]),i("svg",{ref:"offTarget",staticClass:"vocab__off-target",attrs:{viewBox:e.viewBox},on:{click:e.close}},[i("defs",[i("mask",{attrs:{id:"vocab-mask"}},[i("rect",{attrs:{fill:"#ffffff",width:e.width,height:e.height}}),i("rect",{attrs:{fill:"#000000",x:e.termLeft,y:e.termTop,width:e.termWidth,height:e.termHeight}})])]),i("rect",{staticClass:"vocab__off-target-rect",attrs:{x:"0",y:"0",width:"100%",height:"100%"}})])]):e._e()])},ke=[],Ce=5,_e={name:"Vocab",props:{term:{type:String,default:null},position:{type:Object,default:function(){return{x:0,y:0,height:0,width:0}}},terms:{type:Object,default:function(){}},links:{type:Object,default:function(){}}},data:function(){return{width:window.innerWidth,height:window.innerHeight}},computed:{description:function(){return this.term?this.terms[this.term.toLowerCase()]:""},link:function(){if(!this.term)return"";var e=this.term.toLowerCase();return this.links.hasOwnProperty(e)?this.links[e]:"https://en.wikipedia.org/wiki/".concat(this.term)},viewBox:function(){return"0 0 ".concat(this.width," ").concat(this.height)},termWidth:function(){return"".concat(this.position.width,"px")},termHeight:function(){return"".concat(this.position.height+Ce,"px")},termTop:function(){return"".concat(this.position.y-Ce/2,"px")},termLeft:function(){return"".concat(this.position.x,"px")},boxTop:function(){return"".concat(this.position.y+Ce,"px")},boxLeft:function(){return"".concat(this.position.x+this.position.width/2,"px")}},created:function(){window.addEventListener("resize",this.handelResize)},destroyed:function(){window.removeEventListener("resize",this.handelResize)},methods:{handelResize:function(){this.width=window.innerWidth,this.height=window.innerHeight},close:function(e){this.$emit("close",e)}}},Te=_e,Pe=(i("7431"),Object(A["a"])(Te,xe,ke,!1,null,null,null)),Se=Pe.exports,Ae=Se,Ne={name:"App",components:{Cases:M,Columns:O,Description:X,Examples:V,List:oe,Navigator:pe,Step:we,Vocab:Ae},data:function(){return{terms:k,termLinks:C,tree:x,particlesConfig:p,treeDepth:2,treePath:[],stepNodes:[],term:null,scrolled:!1,currentStep:null,throttleHandleKeydown:null,vocabPos:{x:0,y:0,height:0,width:0}}},computed:{steps:function(){return this.treePath.reduce(function(e,t){var i=e[e.length-1],n=i.children[t];return e.concat([n])},[this.tree])},stepsMiddle:function(){return this.stepNodes.map(function(e){return e.offsetTop+e.clientHeight/2})},canReverse:function(){return null!==this.currentStep&&0!==this.currentStep},canProgress:function(){return null===this.currentStep||this.currentStep<this.treePath.length}},watch:{steps:"getStepNodes",currentStep:"setFocus"},created:function(){this.throttleHandleKeydown=d()(this.handleKeydown,500),window.addEventListener("keydown",this.throttleHandleKeydown),window.addEventListener("resize",this.handelResize)},mounted:function(){this.getStepNodes()},destroyed:function(){window.removeEventListener("keydown",this.throttleHandleKeydown),window.removeEventListener("resize",this.handelResize)},methods:{handelResize:function(){if(this.term){var e=document.querySelector(".vocab--active"),t=e.getClientRects()[0],i=t.left,n=t.top,s=t.width,a=t.height;this.vocabPos={x:i,y:n,width:s,height:a}}},handleScroll:function(e){var t=this.$refs.scroller,i=t.scrollTop,n=t.clientHeight,s=i+n/2;this.currentStep=this.closest(s,this.stepsMiddle),this.scrolled=e.currentTarget.scrollTop>70},handleKeydown:function(e){return 13===e.keyCode&&null===this.currentStep?this.goToStep(0):38===e.keyCode&&this.canReverse?this.goToStep(this.currentStep-1):40===e.keyCode&&this.canProgress?this.goToStep(null===this.currentStep?0:this.currentStep+1):void([38,40].includes(e.keyCode)&&e.preventDefault())},selectStep:function(e){var t=this,i=e.parentIdx,n=e.selectedIdx;this.treePath=this.treePath.slice(0,i).concat([n]),setTimeout(function(){return t.goToStep(t.treePath.length)},400)},goToStep:function(e){var t=this,i=arguments.length>1&&void 0!==arguments[1]&&arguments[1];this.$nextTick(function(){var n=document.querySelector('.step[data-index="'.concat(e,'"]')),s=i?t.start:null;l.a.scrollTo(n,{container:t.$refs.scroller,onDone:s}),t.initVocab(n)})},getStepNodes:function(){var e=this;this.$nextTick(function(){var t=document.querySelectorAll(".step");e.stepNodes=[].slice.call(t)})},start:function(){this.treePath=[],this.goToStep(0,!0)},initVocab:function(e){var t=this;if(!e.querySelectorAll(".js-vocab").length){var i=e.querySelectorAll(".description"),n=e.querySelectorAll(".columns"),s=Object(o["a"])(i).concat(Object(o["a"])(n));Object.keys(this.terms).forEach(function(e){s.forEach(function(t){return h()(t,{find:new RegExp('(?:^|["]|[^\\S-])('.concat(e,')(?:$|[.;"]|[^\\S-])'),"i"),wrap:"span",wrapClass:"js-vocab"})});var i=document.querySelectorAll(".js-vocab");i.forEach(function(e){return e.onclick=t.setTerm.bind(t)})})}},setTerm:function(e){var t=e.target;t.classList.add("vocab--active"),this.term=t.textContent.trim().replace(/[.;"]+/g,"");var i=t.getClientRects()[0],n=i.left,s=i.top,a=i.width,o=i.height;this.vocabPos={x:n,y:s,width:a,height:o}},unsetTerm:function(){var e=document.querySelector(".vocab--active");this.term=null,e.classList.remove("vocab--active")},setFocus:function(){var e=document.querySelector('.step[data-index="'.concat(this.currentStep,'"]'));e.focus()},closest:function(e,t){var i=t.reduce(function(t,i){return Math.abs(i-e)<Math.abs(t-e)?i:t},0),n=t.indexOf(i);return-1===n?0:n}}},Re=Ne,Me=(i("5c0b"),Object(A["a"])(Re,s,a,!1,null,null,null)),De=Me.exports,qe=i("002d"),Ee=i.n(qe),Le=i("7a55"),je=i.n(Le),Ie=i("cb5b"),Oe=i("0aa6"),ze=i.n(Oe),Be=i("f2dd"),He=i.n(Be),Fe=i("d54e"),$e=i.n(Fe),Ke=i("63f3"),Ve=i.n(Ke),We=i("1123"),Ge=i.n(We);i("73ec");Ie["default"].library.add(ze.a,He.a,$e.a,Ve.a,Ge.a),n["a"].component("font-awesome-icon",je.a),n["a"].use(Ee.a),n["a"].config.productionTip=!0,new n["a"]({render:function(e){return e(De)}}).$mount("#app")},"5c0b":function(e,t,i){"use strict";var n=i("106f"),s=i.n(n);s.a},"6a03":function(e,t,i){"use strict";var n=i("a27f"),s=i.n(n);s.a},"71d6":function(e){e.exports={particles:{number:{value:100,density:{enable:!0,value_area:800}},color:{value:"#ECECEC"},shape:{type:"circle",stroke:{width:0,color:"#000000"},polygon:{nb_sides:5},image:{src:"img/github.svg",width:100,height:100}},opacity:{value:.4,random:!0,anim:{enable:!0,speed:.07992007992007992,opacity_min:.056845404861094156,sync:!1}},size:{value:3,random:!1,anim:{enable:!1,speed:40,size_min:.1,sync:!1}},line_linked:{enable:!0,distance:150,color:"#8A90A0",opacity:.2,width:1},move:{enable:!0,speed:.5,direction:"none",random:!0,straight:!1,out_mode:"out",bounce:!1,attract:{enable:!0,rotateX:1025.8919341219544,rotateY:1200}}},interactivity:{detect_on:"canvas",events:{onhover:{enable:!1,mode:"grab"},onclick:{enable:!1,mode:"repulse"},resize:!0},modes:{grab:{distance:182.71737276780266,line_linked:{opacity:.1}},bubble:{distance:400,size:40,duration:2,opacity:8,speed:3},repulse:{distance:89.32849335314796,duration:.4},push:{particles_nb:4},remove:{particles_nb:2}}},retina_detect:!0}},"73ec":function(e,t,i){},7431:function(e,t,i){"use strict";var n=i("3eca"),s=i.n(n);s.a},"8b4e":function(e,t,i){"use strict";var n=i("fe48"),s=i.n(n);s.a},a27f:function(e,t,i){},ba17:function(e,t,i){"use strict";var n=i("1640"),s=i.n(n);s.a},c19f:function(e,t,i){"use strict";var n=i("cc4e"),s=i.n(n);s.a},cc4e:function(e,t,i){},ccd2:function(e,t,i){},d0d3:function(e,t,i){},d9a3:function(e){e.exports={hyperplane:"A hyperplane in a 1-dimensional (1D) space is a point. In a 2-dimensional (2D) space, it is a line. A hyperplane in 3-dimensional (3D) space is a plane, a flat surface. To generalize for any dimension, the concept is referred to as a hyperplane.","linear relationships":"A relationship is linear if a change in the first variable corresponds to a constant change in the second variable.","hidden layers":'These are a number of neurons which mathematically transform the data. They are referred to as "hidden" as the user is only concerned with the input layers, where the features are passed, and the output layers, where the prediction is made.',input:"The features are passed as inputs, e.g. size, brand, location, etc.",output:"This is the target variable, the thing we are trying to predict, e.g. the price of an item.","target variable":"This is the thing are are trying to predict, e.g. whether an action is fraudulent or not; the price of a product",observation:"An observation is a single example, a data point or row in the data.",pooling:"This is a way to combine data and is usually done by taking the mean average.",neurons:'An artificial neuron is a mathematical function. It takes one or more inputs that are multiplied by values called "weights" and added together. This value is then passed to a non-linear function, referred to as an "activation function", which becomes the output.',"n-dimensional space":"A 1-dimensional (1D) space is represented simply as a line and 2-dimensional (2D) is referred to as the Cartesian plane, where you can move up or down and right or left. To generalize, n-dimensional space is used.",categories:"The terms categories and classes can be used interchangeably.",independent:"Two features are independent if the value of one does not affect the value of the other. Two events are independent if the probability of one occurring does not affect the probability of the other occurring.",probability:"Probability means to what extend something is likely to happen or be a particular case.",likelihood:"The probability of an event occurring given a criteria can be represented as the likelihood of that criteria given the event occurring.","bayes theorem":"Bayes Theorem is a mathematical formula for determining conditional probability.","binary outcome":"A binary outcome means the variable will be one of two possible values, a 1 or a 0. A 1 indicates that the observation is in the class and a 0 would indicate it isn't.","non-linear relationships":"A non-linear relationship means that the a change in the first variable doesn't necessarily correspond with a constant change in the second. However, they may impact each other but it appears to be unpredictable.","input-class":"The features are passed as inputs, e.g. symptoms, laboratory results, historical diagnosis.","output-class":"This is the target variable, the thing we are trying to predict, e.g. the clinical diagnosis.",model:"Machine learning algorithms create a model after training, this is a mathematical function that can then be used to take a new observation and calculates an appropriate prediction.","threshold-class":"The threshold by default is usually 50%; if the probability for an observation is calculated to be greater than 50% than it is predicted to be in the class.",k:"k is a user-defined value referring to the number of clusters the algorithm should find.",cluster:"A group of similar things that are close together.","normal distribution":"The normal, or Gaussian, distribution is a common probability distribution informally referred to as a bell curve due to its shape.",mean:"Mean is the average calculated by summing up all values and dividing by the number of numbers.","standard deviation":"Standard deviation is the measurement of variance in the data, how spread out the data is.","soft assignment":"Rather than being assigned to one cluster, each point is assigned to all clusters with different probabilities",dense:"Closely compact areas of observations.","hierarchy of clusters":"It is a tree based representation called a Dendrogram where each leaf is an observation and they are combined with branches. The similarity of the observations can be inferred by the height of the branch connecting them.",threshold:"A boundary to use to make a decision. For clustering, this threshold might be used to stop the algorithm after a certain number of merges have occurred or when a certain number of clusters are identified. For classification, if the probability for an observation to be within a class is greater than the threshold, it will be classified as that class",characteristics:"Characteristics are common or similar values seen in the data based upon features, e.g. people that spend a lot of money on rent, those that infrequently make purchases, etc.",boundary:'This is the line, plane or hyperplane that divides the data between those that have been identified as "normal" and those that are not.',"dimensionality reduction":"The initial number of dimension will be the number of features. The goal of dimensionality reduction is to reduce the number of dimensions without losing important information.","neural network":"Neural networks can learn complex patterns using “hidden layers” between inputs and the output. These layers are made of neurons which mathematically transform the data.",vector:"A feature vector is a series of numbers describing the observations characteristics, e.g. brand, words included in the description, price, etc.","distance metric":"A distance metric is a function that calculates the distance between elements, examples include Euclidean distance and cosine distance.","user-based filtering":"User-based filtering recommends products to a user that similar users have liked.","item-based filtering":"Item-based filtering identify similar items based on those previously liked.","bayesian networks":"A Bayesian network is a graphical network where nodes are variables and edges are the conditional dependency between them.","matrix factorization":"In the context of collaborative filtering, matrix factorization is trying to find a matrix for users and a matrix for items that when multiplied approximates the original rating table.",overfitting:"An overfit model will have very high accuracy on the training data, having discovered useful features that are specific in the data it has seen. However, it will have low accuracy on test data as it cannot generalize.","neighborhood-based algorithm":"A similarity measure identifies the most similar users to the user, or most similar items to the user's already-rated items. They are referred to as 'neighborhood' as if you were to plot the data points, these would be the closest",metadata:"Data describing an item, its features. E.g. for a movie, the metadata is its genre, duration, actors, etc.","similarity measure":"A function that quantifies the similarity between objects, e.g. cosine similarity",noise:"Noise refers to data points are incorrect. These are usually identified if they are outliers, which means they are much different to the rest of the data set. However, be cautious as some outliers may be valid data points and worth investigating.","feature engineering":"Feature engineering is the process of transforming the raw data into something more meaningful, this usually involves working with someone that has domain expertise.","high dimensional":"High dimensional data means that the data has a very large number of features. If your data is represented in a CSV, database or Excel file, if there are a lot of columns which you will be using to build a model with, it's high dimensional","cold start problem":"The term 'cold start' is coined from cars not running well when they've been left in the cold. When the recommendation engine doesn't have sufficient data on the user, it doesn't perform very well.",weighting:"If you weight a value, you are assigning an adjustment to it based upon it's important. When pooling, rather than take an average of the values, you can multiple each value by its proportional distance from the item of interest.",parameters:"A parameter is a numerical value that the user can define which is used by the  algorithm; these values impacts the success of the model","hyper-parameters":"A hyper-parameter is a value that is set prior to building a model; these values are important as they impact the success of the model"}},f9b4:function(e,t,i){"use strict";var n=i("5630"),s=i.n(n);s.a},fe48:function(e,t,i){}});
//# sourceMappingURL=app.87a2cd7e.js.map